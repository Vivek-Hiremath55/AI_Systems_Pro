{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVUerOV3RfTr",
    "outputId": "67436ac5-cfc8-4cf4-fd2e-ed8fd399c6da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMporting Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJKPw3xjonfw"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from PIL import Image\n",
    "import numpy as np, pandas as pd\n",
    "import os\n",
    "import io\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIsw2TQ5rDfz"
   },
   "source": [
    "Extracting Images and resizign them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOIfonh8Rb_x"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the path to your .hdf5 dataset file\n",
    "hdf5_file_path = \"/content/drive/MyDrive/AIS_Pro_Data/train-image.hdf5\"\n",
    "\n",
    "# Define the target image size (for example 224x224 for ResNet)\n",
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FJ6x0vGoXqE"
   },
   "outputs": [],
   "source": [
    "hdf5_file = h5py.File(hdf5_file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzWAv9Sdoc94"
   },
   "outputs": [],
   "source": [
    "name_list = list(hdf5_file.keys()) #I'll be using this to access the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvk-1cnJoi-B"
   },
   "outputs": [],
   "source": [
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT7WuslApMg4"
   },
   "outputs": [],
   "source": [
    "# Reading image data and resizing the image so it fits resnet's required size\n",
    "\n",
    "def image_processor(image_name):\n",
    "    image_data = hdf5_file[image_name][()]\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    image_2 = image.resize(target_size)\n",
    "    return image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4El_6v3arODM"
   },
   "outputs": [],
   "source": [
    "def image_displayer(image_name):\n",
    "    image_data = hdf5_file[image_name][()]\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5xzYH0cpYbn"
   },
   "outputs": [],
   "source": [
    "images.clear()\n",
    "for i in range(2):\n",
    "    images.append(image_processor(name_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_fTvT8RrHIW"
   },
   "source": [
    "Extracting labels for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5_eUuKMoc01",
    "outputId": "dbaace84-2a94-44aa-9a35-8f7948983a9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-9acbfeca1fa1>:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_meta = pd.read_csv(\"/content/drive/MyDrive/AIS_Pro_Data/train-metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.read_csv(\"/content/drive/MyDrive/AIS_Pro_Data/train-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRRN80LKryzt",
    "outputId": "d786afc6-f63c-4786-9952-db43229a98a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['isic_id', 'target', 'patient_id', 'age_approx', 'sex',\n",
       "       'anatom_site_general', 'clin_size_long_diam_mm', 'image_type',\n",
       "       'tbp_tile_type', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext',\n",
       "       'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L',\n",
       "       'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio',\n",
       "       'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB',\n",
       "       'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm',\n",
       "       'tbp_lv_eccentricity', 'tbp_lv_location', 'tbp_lv_location_simple',\n",
       "       'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border',\n",
       "       'tbp_lv_norm_color', 'tbp_lv_perimeterMM',\n",
       "       'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt',\n",
       "       'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y',\n",
       "       'tbp_lv_z', 'attribution', 'copyright_license', 'lesion_id',\n",
       "       'iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5',\n",
       "       'mel_mitotic_index', 'mel_thick_mm', 'tbp_lv_dnn_lesion_confidence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpQJRp-8sMRN"
   },
   "outputs": [],
   "source": [
    "needed_column = ['isic_id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPTLNIJ7rdjS"
   },
   "outputs": [],
   "source": [
    "train_data = train_meta[needed_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWZaDLyFrqA6"
   },
   "outputs": [],
   "source": [
    "# Trying out on a smaller dataset\n",
    "\n",
    "td_pos = train_data[train_data['target'] == 1]\n",
    "td_neg = train_data[train_data['target'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goXiG-Vtrp9c",
    "outputId": "cd63fc76-e2e8-4e53-c0e8-42f827772ec9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((393, 2), (400666, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_pos.shape, td_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGnJ4eVhrp6z"
   },
   "outputs": [],
   "source": [
    "shuffled_td_neg = td_neg.sample(random_state=42, n=400666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTmEabAsbCAy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations to make the model robust towards translations and rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DR24qUJbB9Z"
   },
   "outputs": [],
   "source": [
    "from PIL import ImageEnhance, ImageOps\n",
    "\n",
    "def augmented_images(image):\n",
    "    augmentations = []\n",
    "\n",
    "    # Apply rotation\n",
    "    augmentations.append(image.rotate(15))\n",
    "    augmentations.append(image.rotate(-15))\n",
    "\n",
    "    # Flip the image\n",
    "    augmentations.append(ImageOps.mirror(image))\n",
    "\n",
    "    # Adjust brightness\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    augmentations.append(enhancer.enhance(1.5))\n",
    "    augmentations.append(enhancer.enhance(0.7))\n",
    "\n",
    "    # Cropping and scaling back to target size\n",
    "    cropped_image = image.crop((5, 5, image.width - 5, image.height - 5)).resize(target_size)\n",
    "    augmentations.append(cropped_image)\n",
    "\n",
    "    return augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7cafjDkgiQq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y56kJxJDgiIx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzMaoT17giF_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "909LVzESbB3-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mf4HENRUbB0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjHsYEuobBxG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gy8O2Xutrp4M",
    "outputId": "666bbba3-625e-4346-d14e-8b343c45a0c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(786, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_complete = pd.concat([td_pos, shuffled_td_neg])\n",
    "td_complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wCgFeHKzOJg"
   },
   "outputs": [],
   "source": [
    "# Accessing images using the image names from td_complete\n",
    "images.clear()\n",
    "for i in range(td_complete.shape[0]):\n",
    "    images.append(image_processor(td_complete['isic_id'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMMsBBhqAwFH",
    "outputId": "f282b66f-2f05-4828-f182-dd82552789a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M25HLuNrbB6H"
   },
   "outputs": [],
   "source": [
    "# Modifying the minority images\n",
    "\n",
    "for i in range(len(td_pos)):  # Apply to minority class only\n",
    "    image = image_processor(td_pos['isic_id'].iloc[i])\n",
    "   # images.append(image)  # Original image\n",
    "    images.extend(augmented_images(image))  # Add augmented versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ST1pS1iiz7Y",
    "outputId": "f282b66f-2f05-4828-f182-dd82552789a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOlLm-bYi4ev"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTFOTb5ui4bO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfDjml2Ti4Pf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9q4ia7Ydi4MU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtO64Dcuyoex"
   },
   "outputs": [],
   "source": [
    "X = images\n",
    "y = td_complete['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Odtkyy9Qi7dn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations on the images to format it into model's required size, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6HfLV3GyyVY"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# Define transformations for your images to match ResNet input requirements\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet requires 224x224 input size\n",
    "    transforms.ToTensor(),  # Convert to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "# Apply transformation to each image in your list\n",
    "X_transformed = [transform(image) for image in images]\n",
    "\n",
    "# Stack the list into a single tensor\n",
    "X_tensor = torch.stack(X_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YynGo5Y0BNAs"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert target labels into a tensor\n",
    "y_tensor = torch.tensor(y.values)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIa2aqkmBc3f",
    "outputId": "8c198e9c-937f-4b88-9286-3f0f4165432e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 71.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Load ResNet18 model\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final layer to output 2 classes (skin cancer - binary classification)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # For binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are relying on crossentropy as our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FuBpp8kBokP"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tzWoeR7BsIz",
    "outputId": "4efbed8d-e35f-4ce5-bb6b-1492601d5d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8032058608531952\n",
      "Epoch 2, Loss: 0.4678682720661163\n",
      "Epoch 3, Loss: 0.3714549207687378\n",
      "Epoch 4, Loss: 0.36422820210456847\n",
      "Epoch 5, Loss: 0.2817980831861496\n",
      "Epoch 6, Loss: 0.2512885445356369\n",
      "Epoch 7, Loss: 0.20160607486963272\n",
      "Epoch 8, Loss: 0.1759279029816389\n",
      "Epoch 9, Loss: 0.2221507152915001\n",
      "Epoch 10, Loss: 0.15593516692519188\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(10):  # Set the number of epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJ6reP39ByO7",
    "outputId": "2d0a4a08-2236-4f44-edc7-353ca7e7297f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.78371501272265%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojPRHRsAKmby"
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_cbTdomKoiq",
    "outputId": "c4f848dc-bd1e-4b66-ca26-f35a01728549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_classifier.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'image_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1axvk-xKSLx",
    "outputId": "10e42ab2-60d0-49cc-e939-9bb32eea59e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['isic_id', 'target'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_complete.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXfu62T3Kbhc"
   },
   "outputs": [],
   "source": [
    "classifier = joblib.load('image_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3L5WuTfbfP_V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
