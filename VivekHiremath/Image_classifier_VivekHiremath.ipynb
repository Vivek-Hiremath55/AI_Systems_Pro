{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbke89Yswun7dku+g7nwxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivek-Hiremath55/AI_Systems_Pro/blob/main/VivekHiremath/Image_classifier_VivekHiremath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVUerOV3RfTr",
        "outputId": "16973ac8-b526-47da-a87a-7ab07ccea33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "from PIL import Image\n",
        "import numpy as np, pandas as pd\n",
        "import os\n",
        "import io\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "JJKPw3xjonfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Images and resizign them"
      ],
      "metadata": {
        "id": "UIsw2TQ5rDfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOIfonh8Rb_x"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the path to your .hdf5 dataset file\n",
        "hdf5_file_path = \"/content/drive/MyDrive/AIS_Pro_Data/train-image.hdf5\"\n",
        "\n",
        "# Define the target image size (for example 224x224 for ResNet)\n",
        "target_size = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hdf5_file = h5py.File(hdf5_file_path, 'r')"
      ],
      "metadata": {
        "id": "2FJ6x0vGoXqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_list = list(hdf5_file.keys()) #I'll be using this to access the images"
      ],
      "metadata": {
        "id": "mzWAv9Sdoc94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []"
      ],
      "metadata": {
        "id": "rvk-1cnJoi-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading image data and resizing the image so it fits resnet's required size\n",
        "\n",
        "def image_processor(image_name):\n",
        "    image_data = hdf5_file[image_name][()]\n",
        "    image = Image.open(io.BytesIO(image_data))\n",
        "    image_2 = image.resize(target_size)\n",
        "    return image_2"
      ],
      "metadata": {
        "id": "IT7WuslApMg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_displayer(image_name):\n",
        "    image_data = hdf5_file[image_name][()]\n",
        "    image = Image.open(io.BytesIO(image_data))\n",
        "    display(image)"
      ],
      "metadata": {
        "id": "4El_6v3arODM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.clear()\n",
        "for i in range(2):\n",
        "    images.append(image_processor(name_list[i]))"
      ],
      "metadata": {
        "id": "d5xzYH0cpYbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting labels for images"
      ],
      "metadata": {
        "id": "e_fTvT8RrHIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_meta = pd.read_csv(\"/content/drive/MyDrive/AIS_Pro_Data/train-metadata.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5_eUuKMoc01",
        "outputId": "2934005b-791a-437f-a1db-8c8bd92899a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-9acbfeca1fa1>:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_meta = pd.read_csv(\"/content/drive/MyDrive/AIS_Pro_Data/train-metadata.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_meta.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRRN80LKryzt",
        "outputId": "d786afc6-f63c-4786-9952-db43229a98a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['isic_id', 'target', 'patient_id', 'age_approx', 'sex',\n",
              "       'anatom_site_general', 'clin_size_long_diam_mm', 'image_type',\n",
              "       'tbp_tile_type', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext',\n",
              "       'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L',\n",
              "       'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio',\n",
              "       'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB',\n",
              "       'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm',\n",
              "       'tbp_lv_eccentricity', 'tbp_lv_location', 'tbp_lv_location_simple',\n",
              "       'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border',\n",
              "       'tbp_lv_norm_color', 'tbp_lv_perimeterMM',\n",
              "       'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt',\n",
              "       'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y',\n",
              "       'tbp_lv_z', 'attribution', 'copyright_license', 'lesion_id',\n",
              "       'iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5',\n",
              "       'mel_mitotic_index', 'mel_thick_mm', 'tbp_lv_dnn_lesion_confidence'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "needed_column = ['isic_id', 'target']"
      ],
      "metadata": {
        "id": "RpQJRp-8sMRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_meta[needed_column]"
      ],
      "metadata": {
        "id": "FPTLNIJ7rdjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying out on a smaller dataset\n",
        "\n",
        "td_pos = train_data[train_data['target'] == 1]\n",
        "td_neg = train_data[train_data['target'] == 0]\n"
      ],
      "metadata": {
        "id": "HWZaDLyFrqA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td_pos.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goXiG-Vtrp9c",
        "outputId": "7d1978ad-d149-4796-830d-a707d18ba653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(393, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_td_neg = td_neg.sample(random_state=42, n=393)"
      ],
      "metadata": {
        "id": "pGnJ4eVhrp6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td_complete = pd.concat([td_pos, shuffled_td_neg])\n",
        "td_complete.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy8O2Xutrp4M",
        "outputId": "666bbba3-625e-4346-d14e-8b343c45a0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(786, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing images using the image names from td_complete\n",
        "images.clear()\n",
        "for i in range(td_complete.shape[0]):\n",
        "    images.append(image_processor(td_complete['isic_id'].iloc[i]))"
      ],
      "metadata": {
        "id": "7wCgFeHKzOJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMMsBBhqAwFH",
        "outputId": "f282b66f-2f05-4828-f182-dd82552789a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "786"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = images\n",
        "y = td_complete['target']"
      ],
      "metadata": {
        "id": "MtO64Dcuyoex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Define transformations for your images to match ResNet input requirements\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ResNet requires 224x224 input size\n",
        "    transforms.ToTensor(),  # Convert to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
        "])\n",
        "\n",
        "# Apply transformation to each image in your list\n",
        "X_transformed = [transform(image) for image in images]\n",
        "\n",
        "# Stack the list into a single tensor\n",
        "X_tensor = torch.stack(X_transformed)\n"
      ],
      "metadata": {
        "id": "a6HfLV3GyyVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert target labels into a tensor\n",
        "y_tensor = torch.tensor(y.values)\n",
        "\n",
        "# Create a dataset\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "YynGo5Y0BNAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# Load ResNet18 model\n",
        "model = resnet18(pretrained=True)\n",
        "\n",
        "# Modify the final layer to output 2 classes (skin cancer - binary classification)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)  # For binary classification\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIa2aqkmBc3f",
        "outputId": "8c198e9c-937f-4b88-9286-3f0f4165432e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 71.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # Loss for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n"
      ],
      "metadata": {
        "id": "_FuBpp8kBokP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):  # Set the number of epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tzWoeR7BsIz",
        "outputId": "4efbed8d-e35f-4ce5-bb6b-1492601d5d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8032058608531952\n",
            "Epoch 2, Loss: 0.4678682720661163\n",
            "Epoch 3, Loss: 0.3714549207687378\n",
            "Epoch 4, Loss: 0.36422820210456847\n",
            "Epoch 5, Loss: 0.2817980831861496\n",
            "Epoch 6, Loss: 0.2512885445356369\n",
            "Epoch 7, Loss: 0.20160607486963272\n",
            "Epoch 8, Loss: 0.1759279029816389\n",
            "Epoch 9, Loss: 0.2221507152915001\n",
            "Epoch 10, Loss: 0.15593516692519188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy: {100 * correct / total}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ6reP39ByO7",
        "outputId": "2d0a4a08-2236-4f44-edc7-353ca7e7297f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.78371501272265%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "ojPRHRsAKmby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model, 'image_classifier.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_cbTdomKoiq",
        "outputId": "c4f848dc-bd1e-4b66-ca26-f35a01728549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['image_classifier.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "td_complete.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1axvk-xKSLx",
        "outputId": "10e42ab2-60d0-49cc-e939-9bb32eea59e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['isic_id', 'target'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXfu62T3Kbhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}